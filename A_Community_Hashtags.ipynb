{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "\n",
    "import pprint\n",
    "import glob\n",
    "from random import random, randrange\n",
    "from random import choice\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "from scipy.signal import savgol_filter\n",
    "import itertools\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the regular dictionary\n",
    "f = open('pruned_retweet_dict','rb')\n",
    "retweet_dict = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "PT = nx.read_gexf('PT-pruned.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters we care about\n",
    "clusters = [13,48,6,49,104]\n",
    "cluster_names = ['Democrats',\"Republicans\",\"Unorthodox\",\"Public Health\",\"Antivaxxers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_to_cluster(author):\n",
    "    return(PT.nodes[author]['louvain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mininglist1 = ['vax', 'vaxxed', 'vaccine', 'vaccination', 'vaccinations', \n",
    "'vaxsafety', 'vax saftey', 'vaccineswork', 'vaccines work', 'vaccinesaftey',\n",
    "'vaccine saftey', 'vaccines revealed', 'vaccinesrevealed', \n",
    "'novax', 'no vax', 'no-vax', 'antivax', 'anti-vax', 'anti vax', 'immunisation', \n",
    "'Vaccin', 'Vaccinaties', 'vaccinatiezorg', 'vaccine injury', 'vax injury',\n",
    "'vaccinatieschade']\n",
    "\n",
    "extralist = ['vaccines','coronavirus','corona','covid19','virus','vaccine','covid','covid-19',\\\n",
    "             'covidー19','#covidー19','vaccin','im','covid19','would',\\\n",
    "            'dont', 'year', 'going', 'months', 'first', 'new','covid_19','africansarenotlabrats',\\\n",
    "            'wwg1wga','sarscov2','covid2019','wwgwga']\n",
    "\n",
    "remove_terms = np.array([])\n",
    "remove_terms = np.append(remove_terms,mininglist1)\n",
    "#remove_terms = np.append(remove_terms,mininglist2)\n",
    "remove_terms = np.append(remove_terms,extralist)\n",
    "remove_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Def Remove emoji from string\n",
    "def deEmojify(s):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r' ',s)\n",
    "\n",
    "\n",
    "# Def remove arabic\n",
    "# According to Wikipedia, Arabic characters fall in the Unicode range 0600 - 06FF. \n",
    "def remove_foreign(s): \n",
    "    arabic_list = re.findall(r'[\\u0600-\\u06FF]+',s)\n",
    "    for w in arabic_list:\n",
    "        s = s.replace(w, \" \")\n",
    "        \n",
    "    chinese_list = re.findall(r'[\\u4e00-\\u9FFF]+',s)\n",
    "    for w in chinese_list:\n",
    "        s = s.replace(w, \" \")\n",
    "        \n",
    "    hindi_list = re.findall(r'[\\u0900-\\u097F]+',s)\n",
    "    for w in hindi_list:\n",
    "        s = s.replace(w, \" \")\n",
    "             \n",
    "    korean_list = re.findall(r'[\\u3040-\\u30ff]+',s)\n",
    "    for w in korean_list:\n",
    "        s = s.replace(w, \" \")\n",
    "              \n",
    "    japanese_list = re.findall(r'[\\u3040-\\u30ff]+',s)\n",
    "    for w in japanese_list:\n",
    "        s = s.replace(w, \" \")\n",
    "        \n",
    "    return(s)\n",
    "\n",
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "def light_preprocessor(s):\n",
    "    s = s.strip().lower()\n",
    "    s = s.replace('\\n',\" \")\n",
    "    s = s.replace('-', \" \")\n",
    "    s = s.replace('.', \"\")\n",
    "    # Remove Emojis\n",
    "    s = deEmojify(s)\n",
    "    # Remove Arabic\n",
    "    s = remove_foreign(s)\n",
    "    \n",
    "    s = re.sub(\"\\s+\",\" \",s)\n",
    "    \n",
    "    s = s.translate(remove_digits)\n",
    "    \n",
    "    s = [x for x in s if (x.isalnum() or x == \" \" or x==\"#\" or x==\"@\")]\n",
    "    s = \"\".join(s)\n",
    "     # Remove stopwords in english and french\n",
    "    stop_words = stopwords.words('english') + stopwords.words('french')\n",
    "    s = \" \".join([x for x in s.split() if ((x not in stop_words) \n",
    "                                           and (x.find('http') == -1) \n",
    "                                           #and (x[0] != '#')\n",
    "                                           #and (x[0] != '@')\n",
    "                                          and (x != 'amp')\n",
    "                                         and (x not in remove_terms))])\n",
    "    s = s.replace('/',' ')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unfiltered version:\")\n",
    "for item in retweet_dict[1212147705507700736]['hashtags']:\n",
    "    #item = item.lower()\n",
    "    #if item not in remove_terms:\n",
    "    print(item)\n",
    "\n",
    "print(\"Filtered version:\")\n",
    "for item in retweet_dict[1212147705507700736]['hashtags']:\n",
    "    item = light_preprocessor(item)\n",
    "    print(\"'\" + item + \"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hashtag_corpus(date1,date2):\n",
    "    date1 = datetime.strptime(date1,\"%d/%m/%y\")\n",
    "    date2 = datetime.strptime(date2,\"%d/%m/%y\")\n",
    "    corpus = dict()\n",
    "    for c in clusters:\n",
    "        corpus[str(c)]=\"\"\n",
    "\n",
    "    for t in tqdm(retweet_dict):\n",
    "        a = retweet_dict[t]['author']\n",
    "        try:\n",
    "            c = author_to_cluster(a)\n",
    "            if c in clusters:\n",
    "                rt_date = datetime.strptime(retweet_dict[t]['date'],'%y-%m-%d-%H:%M:%S')\n",
    "                if (date1 <= rt_date) and (rt_date <= date2):\n",
    "                    hashtag_list = retweet_dict[t]['hashtags']\n",
    "                    for hashtag in hashtag_list:\n",
    "                        hashtag = light_preprocessor(hashtag)\n",
    "                        text = \" \" + hashtag\n",
    "                    corpus[str(c)] += text\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    # remove double spaces\n",
    "    for key in corpus:\n",
    "        for i in range(4):\n",
    "            corpus[key] = corpus[key].replace('  ', \" \")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test = create_hashtag_corpus('26/12/19','28/12/19')\n",
    "test['104']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_corpus = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "if create_corpus:\n",
    "    corpus = create_hashtag_corpus('26/12/19','27/05/20')\n",
    "    f = open(\"hastag_corpus.pkl\",\"wb\")\n",
    "    pickle.dump(corpus,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('hastag_corpus.pkl','rb')\n",
    "corpus = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_corpus = []\n",
    "for cluster in clusters:\n",
    "    true_corpus.append(corpus[str(cluster)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.5,max_df=0.9,lowercase=True)\n",
    "tfidf = vectorizer.fit_transform(true_corpus)\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# To get the cool hashtags:\n",
    "# vectorizer = TfidfVectorizer(min_df=0.5,max_df=0.9,lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See second answer of:\n",
    "# https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "def get_top_tf_idf_words(response, top_n=2):\n",
    "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
    "    return feature_names[response.indices[sorted_nzs]]\n",
    "  \n",
    "print([get_top_tf_idf_words(response,15) for response in tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
